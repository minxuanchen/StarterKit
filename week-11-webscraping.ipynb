{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setup**\n",
    "\n",
    "1. Install chromium browser, beautifulsoup and requests\n",
    "    1. sudo apt-get install chromium-browser \n",
    "    + conda install beautifulsoup4\n",
    "    + conda install requests\n",
    "+ Make sure all the files are available in the folder that you are working with "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Slide 5\n",
    "# Open \"Files\" file organizer\n",
    "# Right-click on \"simple.html\"\n",
    "# Right-click on document and \"inspect\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#path = \"C:/Users/kanungo/odrive/Google Drive/mydata/Courses/Programming for analytics/Fall 2017/Slides PPT/Week 11 - Web scraping/code and materials/files/\"\n",
    "#path = \"/home/drk/kanungo/DNSC6211/week11/code and materials/files/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Slide 17\n",
    "import urllib.request\n",
    "url = \"http://www.google.com/\"\n",
    "request = urllib.request.Request(url)\n",
    "response = urllib.request.urlopen(request)\n",
    "print (response.info())\n",
    "html = response.read()\n",
    "# print (html)                   #  <-- Try this out \n",
    "response.close()                 # best to close the file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slide 18\n",
    "import urllib.request\n",
    "filename = \"simple.html\"\n",
    "url = \"file:///\"+path+filename\n",
    "request = urllib.request.Request(url)\n",
    "response = urllib.request.urlopen(request)\n",
    "data = response.read()\n",
    "print (data)\n",
    "response.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Slide 20\n",
    "# Next few cells\n",
    "# Before you go to the next few cells\n",
    "#    Go to www.gwu.edu\n",
    "#    Explore the page using inspect (in any browser)\n",
    "#    Look for all the <a> tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r = requests.get(\"http://www.gwu.edu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = r.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "soup = bs(data, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for link in soup.find_all('a'):\n",
    "    print(link.get('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Slide 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Slide 22\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "filename = \"simple.html\"\n",
    "url = \"file:///\"+path+filename\n",
    "request = urllib.request.Request(url)\n",
    "response = urllib.request.urlopen(request)\n",
    "data = response.read()\n",
    "response.close()\n",
    "\n",
    "# Create the soup\n",
    "soup = bs(data, \"html.parser\")\n",
    "\n",
    "# Print parse tree\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Slide 22\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "filename = \"multipleparas.html\"\n",
    "url = \"file:///\"+path+filename\n",
    "request = urllib.request.Request(url)\n",
    "response = urllib.request.urlopen(request)\n",
    "data = response.read()\n",
    "response.close()\n",
    "\n",
    "# Create the soup\n",
    "soup = bs(data, \"html.parser\")\n",
    "\n",
    "a = soup.find('p')\n",
    "b = soup.findAll('p')\n",
    "\n",
    "print(len(a))\n",
    "print(len(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Slide 25\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "filename = \"third.html\"\n",
    "url = \"file:///\"+path+filename\n",
    "request = urllib.request.Request(url)\n",
    "response = urllib.request.urlopen(request)\n",
    "data = response.read()\n",
    "response.close()\n",
    "\n",
    "# Create the soup\n",
    "soup = bs(data, \"html.parser\")\n",
    "\n",
    "soup.li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Slide 25\n",
    "type(soup.li)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Slide 25\n",
    "tag = soup.li\n",
    "tag.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Slide 25\n",
    "tag.attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Slide 26\n",
    "soup.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Slide 26\n",
    "soup.head.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Slide 26\n",
    "soup.li.contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Slide 27\n",
    "soup.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Slide 27\n",
    "type(soup.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Slide 27\n",
    "soup.title.string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Slide 27\n",
    "type(soup.title.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Slide 27\n",
    "soup.title.string.parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Slide 27\n",
    "str(soup.title.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Slide 28\n",
    "# Please explore slide 28 by inspecting it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Slide 29\n",
    "# Do in class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Slide 30\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "filename = \"table.html\"\n",
    "url = \"file:///\"+path+filename\n",
    "request = urllib.request.Request(url)\n",
    "response = urllib.request.urlopen(request)\n",
    "data = response.read()\n",
    "response.close()\n",
    "\n",
    "# Create the soup\n",
    "soup = bs(data, \"html.parser\")\n",
    "\n",
    "# ---- Now we start parsing the table\n",
    "\n",
    "tables = soup.findAll('table')\n",
    "\n",
    "for table in tables:\n",
    "    rows = table.findAll('tr')\n",
    "    for row in rows:\n",
    "        cells = row.findAll('td')\n",
    "        for cell in cells:\n",
    "            print (cell.getText())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Slide 31\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "filename = \"table.html\"\n",
    "url = \"file:///\"+path+filename\n",
    "request = urllib.request.Request(url)\n",
    "response = urllib.request.urlopen(request)\n",
    "data = response.read()\n",
    "response.close()\n",
    "\n",
    "# Create the soup\n",
    "soup = bs(data, \"html.parser\")\n",
    "\n",
    "# ---- Now we start parsing the table\n",
    "\n",
    "tables = soup.findAll('table')\n",
    "\n",
    "myLists = []\n",
    "\n",
    "for table in tables:\n",
    "    rows = table.findAll('tr')\n",
    "    # print \"number of rows =\", len(rows)\n",
    "    r = []\n",
    "    for row in rows:\n",
    "        cells = row.findAll('td')\n",
    "        # print \"number of cols =\", len(cells)\n",
    "        c = []        \n",
    "        for cell in cells:\n",
    "            c.append(cell.getText())\n",
    "        # print c\n",
    "        r.append(c)\n",
    "    myLists.append(r)\n",
    "\n",
    "print (myLists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Slide 31\n",
    "# This part shows how you get Pandas dataframes from lists\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "headings = ['Heading1', 'Heading2','Heading3']\n",
    "\n",
    "df1 = pd.DataFrame(myLists[0], columns=headings)\n",
    "df2 = pd.DataFrame(myLists[1], columns=headings)\n",
    "\n",
    "print(df1)\n",
    "\n",
    "print(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slide 32\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "\n",
    "url=\"http://www.usatoday.com/sports/nfl/rankings/\"\n",
    "r = requests.get(url)\n",
    "data = r.text    \n",
    "soup = bs(data, \"html.parser\")\n",
    "\n",
    "rows = soup.find(\"table\").find(\"tbody\").find_all(\"tr\")\n",
    "print(rows)\n",
    "\n",
    "teams = soup.findAll('table',{'class':'ribbonfx rankings no-autorank'})\n",
    "print(\"length of teams\", len(teams))\n",
    "\n",
    "for team in teams:\n",
    "    rows = team.findAll('tr')\n",
    "    for row in rows:\n",
    "        teamRank = row.find('td',{'class':'ranking'})\n",
    "        teamSlug = row.findAll('td',{'class':'team_slug'})\n",
    "        teamRcrd = row.findAll('td',{'class':'record'})\n",
    "        teamDiff = row.findAll('td',{'class':'ranking diff'})\n",
    "        teamHilo = row.findAll('td',{'class':'ranking hilo'})\n",
    "        teamNote = row.findAll('td',{'class':'notes'})\n",
    "        print(type(teamRank), \" \", teamRank)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Slide 32\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "\n",
    "url=\"http://www.usatoday.com/sports/nfl/rankings/\"\n",
    "r = requests.get(url)\n",
    "data = r.text    \n",
    "soup = bs(data, \"html.parser\")\n",
    "\n",
    "rows = soup.find(\"table\").find(\"tbody\").findAll(\"tr\")\n",
    "\n",
    "for row in rows:\n",
    "    teamRank = row.find('td',{'class':'ranking'})\n",
    "    teamSlug = row.findAll('td',{'class':'team_slug'})\n",
    "    teamRcrd = row.findAll('td',{'class':'record'})\n",
    "    teamDiff = row.findAll('td',{'class':'ranking diff'})\n",
    "    teamHilo = row.findAll('td',{'class':'ranking hilo'})\n",
    "    teamNote = row.find('td',{'class':'notes'})\n",
    "    print(type(teamRank))\n",
    "    #print(teamRank.getText(), \" ------->>   \", str(teamNote))\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
