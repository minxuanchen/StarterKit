{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The notebook for Project 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem1 Part A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* remove old version files\n",
    "* download holmes.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'holmes.txt': No such file or directory\n",
      "--2017-09-17 19:07:24--  https://s3.amazonaws.com/2017-dmfa/project-1/holmes.txt\n",
      "Resolving s3.amazonaws.com (s3.amazonaws.com)... 54.231.72.130\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|54.231.72.130|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 581878 (568K) [text/plain]\n",
      "Saving to: ‘holmes.txt’\n",
      "\n",
      "holmes.txt          100%[===================>] 568.24K  --.-KB/s    in 0.03s   \n",
      "\n",
      "2017-09-17 19:07:24 (16.2 MB/s) - ‘holmes.txt’ saved [581878/581878]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!rm holmes.txt\n",
    "!wget https://s3.amazonaws.com/2017-dmfa/project-1/holmes.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * have a look at this txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!cat holmes.txt | head -20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Names should come with upper case and lower case, so we don't need to tranfer case.\n",
    "* To search the names, we might need grep function.\n",
    "* Count the lines that contain Holmes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!grep -c Holmes holmes.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* But a line may contain more then a name more than once, so split up the text to find names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!cat holmes.txt | tr -sc '[:alpha:]' '[\\n*]' | grep -c Holmes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Output the result with one line command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!cat holmes.txt | tr -sc '[:alpha:]' '[\\n*]' | sort | uniq -c | grep -e Holmes -e Watson -e Adler -e Hudson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The answer for P1-part A is that Holmes shows 461 times, Watson shows 81 times, Adler shows 15times and Hudson shows 4 times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem1 Part B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* remove old version files\n",
    "* get the txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!rm romeo.txt\n",
    "!wget https://s3.amazonaws.com/2017-dmfa/project-1/romeo.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!cat romeo.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Given that this file is the text of a play, after a glance of this txt, We can find that all speaking lines start with \"Name. \".\n",
    "* Romeo's speaking line begins with \"Rom. \"\n",
    "* Juliet's speaking line begins with \"Jul. \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!grep -c \"Jul. \" romeo.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!grep -c \"Rom. \" romeo.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem2 Part A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!wget https://s3.amazonaws.com/2017-dmfa/project-1/2016-Q3-cabi-trips-history-data.zip\n",
    "!rm 2016-Q3-cabi-trips-history-data.zip.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!unzip 2016-Q3-cabi-trips-history-data.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Use csvstack to merge the 2 csv files to data.csv\n",
    "* may need 20s or more!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!csvstack 2016-Q3-Trips-History-Data-1.csv 2016-Q3-Trips-History-Data-2.csv > data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!head -5 data.csv | csvlook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 5th column is the start station.\n",
    "* So the top 10 most popular departing stations are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  20124 Columbus Circle / Union Station\r\n",
      "  19581 Lincoln Memorial\r\n",
      "  19103 Jefferson Dr & 14th St SW\r\n",
      "  16099 Massachusetts Ave & Dupont Circle NW\r\n",
      "  13134 15th & P St NW\r\n",
      "  12984 Henry Bacon Dr & Lincoln Memorial Circle NW\r\n",
      "  12519 Jefferson Memorial\r\n",
      "  11469 Thomas Circle\r\n",
      "  10762 14th & V St NW\r\n",
      "   9767 New Hampshire Ave & T St NW\r\n"
     ]
    }
   ],
   "source": [
    "!csvcut -c5 data.csv | csvsort | uniq -c | sort -n -r | head -10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 7th column is the destination station.\n",
    "* So the top 10 most popular destination stations are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  20814 Columbus Circle / Union Station\r\n",
      "  19624 Lincoln Memorial\r\n",
      "  19296 Jefferson Dr & 14th St SW\r\n",
      "  17138 Massachusetts Ave & Dupont Circle NW\r\n",
      "  13845 15th & P St NW\r\n",
      "  12925 Jefferson Memorial\r\n",
      "  12664 Henry Bacon Dr & Lincoln Memorial Circle NW\r\n",
      "  11657 14th & V St NW\r\n",
      "  10987 Thomas Circle\r\n",
      "  10204 New Hampshire Ave & T St NW\r\n"
     ]
    }
   ],
   "source": [
    "!csvcut -c7 data.csv | csvsort | uniq -c | sort -n -r | head -10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem2 Part B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The most popular departure station is \n",
    "* For that station, the top 10 most used bikes are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     23 W01092\r\n",
      "     21 W22623\r\n",
      "     19 W22487\r\n",
      "     18 W20223\r\n",
      "     17 W22786\r\n",
      "     17 W22753\r\n",
      "     17 W21732\r\n",
      "     16 W22903\r\n",
      "     16 W22591\r\n",
      "     16 W22567\r\n",
      "sort: write failed: 'standard output': Broken pipe\r\n",
      "sort: write error\r\n"
     ]
    }
   ],
   "source": [
    "!csvcut -c5,8 data.csv | csvgrep -c 1 -m \"Columbus Circle / Union Station\" | csvcut -c2 | csvsort | uniq -c |sort -n -r| head -10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The most popular destination station is \n",
    "* For that station, the top 10 most used bikes are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     22 W01092\r\n",
      "     20 W22623\r\n",
      "     19 W20223\r\n",
      "     17 W22487\r\n",
      "     17 W22377\r\n",
      "     17 W21907\r\n",
      "     17 W21285\r\n",
      "     17 W00319\r\n",
      "     16 W22753\r\n",
      "     16 W22567\r\n",
      "sort: write failed: 'standard output': Broken pipe\r\n",
      "sort: write error\r\n"
     ]
    }
   ],
   "source": [
    "!csvcut -c7,8 data.csv | csvgrep -c 1 -m \"Columbus Circle / Union Station\" | csvcut -c2 | csvsort | uniq -c |sort -n -r| head -10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3 Part A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2017-09-17 16:58:32--  https://s3.amazonaws.com/2017-dmfa/project-1/simplefilter.py\n",
      "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.65.3\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.65.3|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 208 [binary/octet-stream]\n",
      "Saving to: ‘simplefilter.py’\n",
      "\n",
      "simplefilter.py     100%[===================>]     208  --.-KB/s    in 0s      \n",
      "\n",
      "2017-09-17 16:58:32 (7.41 MB/s) - ‘simplefilter.py’ saved [208/208]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://s3.amazonaws.com/2017-dmfa/project-1/simplefilter.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A typical pipeline that performs a count of the top ten most common words in Sherlock Holmes is like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   5810 the\r\n",
      "   3088 and\r\n",
      "   3038 i\r\n",
      "   2823 to\r\n",
      "   2778 of\r\n",
      "   2701 a\r\n",
      "   1823 in\r\n",
      "   1767 that\r\n",
      "   1749 it\r\n",
      "   1572 you\r\n",
      "sort: write failed: 'standard output': Broken pipe\r\n",
      "sort: write error\r\n"
     ]
    }
   ],
   "source": [
    "!cat holmes.txt | tr -sc '[:alpha:]' '[\\n*]'| tr '[:upper:]' '[:lower:]' | sort | uniq -c | sort -rn | head -10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* That's how this pipeline works:\n",
    "* !cat holmes.txt                ----firstly, get the content of holmes.txt, pump the content to \"tr\" function;\n",
    "* | tr -sc '[:alpha:]' '[\\n]'        ----replace everything except english characters with \"\\n\"(means go to next line),\n",
    "*                           ----this split lines of text into one word per line, output the result to another \"tr\";\n",
    "* | tr '[:upper:]' '[:lower:]'       ----tranfer all upper case to lower case so that the sort function can recognize all the words;\n",
    "* | sort                      ----sort the words and put the same words together;\n",
    "* | uniq -c                    ----find unique words and count their duplicated times;\n",
    "* | sort -rn                   ----sort the counting results by decreasing number order;\n",
    "* | head -10                   ----get the biggest 10 numbers and corresponding words, and they are the top 10 most common words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let's write our own filters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x simplefilter.py\n",
    "!chmod +x split.py\n",
    "!chmod +x case.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Go to terminal and edit the filter with nano\n",
    "* use lower() and split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/usr/bin/env python\r\n",
      "\r\n",
      "\"\"\"\r\n",
      "A filter that transforms text into lower case.\r\n",
      "\"\"\"\r\n",
      "\r\n",
      "import fileinput\r\n",
      "\r\n",
      "\r\n",
      "def process(line):\r\n",
      "    \"\"\"For each line of input, _____.\"\"\"\r\n",
      "    print(line[:-1].lower())\r\n",
      "\r\n",
      "\r\n",
      "for line in fileinput.input():\r\n",
      "    process(line)\r\n"
     ]
    }
   ],
   "source": [
    "!cat case.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/usr/bin/env python\r\n",
      "\r\n",
      "\"\"\"\r\n",
      "A filter that split lines into one word per line.\r\n",
      "\"\"\"\r\n",
      "\r\n",
      "import fileinput\r\n",
      "\r\n",
      "\r\n",
      "def process(line):\r\n",
      "    \"\"\"For each line of input, split it into words.\"\"\"\r\n",
      "    x=line.split()\r\n",
      "    for i in x :\r\n",
      "        print(i)\r\n",
      "\r\n",
      "\r\n",
      "for line in fileinput.input():\r\n",
      "    process(line)\r\n"
     ]
    }
   ],
   "source": [
    "!cat split.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the project gutenberg ebook of romeo and juliet, by william shakespeare\r\n",
      "\r\n",
      "this ebook is for the use of anyone anywhere at no cost and with\r\n",
      "almost no restrictions whatsoever.  you may copy it, give it away or\r\n",
      "re-use it under the terms of the project gutenberg license included\r\n",
      "with this ebook or online at www.gutenberg.org/license\r\n",
      "\r\n",
      "\r\n",
      "title: romeo and juliet\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!head -10 romeo.txt | /home/ubuntu/Project1/case.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The\r\n",
      "Project\r\n",
      "Gutenberg\r\n",
      "EBook\r\n",
      "of\r\n",
      "Romeo\r\n",
      "and\r\n",
      "Juliet,\r\n",
      "by\r\n",
      "William\r\n",
      "Shakespeare\r\n",
      "This\r\n",
      "eBook\r\n",
      "is\r\n",
      "for\r\n",
      "the\r\n",
      "use\r\n",
      "of\r\n",
      "anyone\r\n",
      "anywhere\r\n",
      "at\r\n",
      "no\r\n",
      "cost\r\n",
      "and\r\n",
      "with\r\n",
      "almost\r\n",
      "no\r\n",
      "restrictions\r\n",
      "whatsoever.\r\n",
      "You\r\n",
      "may\r\n",
      "copy\r\n",
      "it,\r\n",
      "give\r\n",
      "it\r\n",
      "away\r\n",
      "or\r\n",
      "re-use\r\n",
      "it\r\n",
      "under\r\n",
      "the\r\n",
      "terms\r\n",
      "of\r\n",
      "the\r\n",
      "Project\r\n",
      "Gutenberg\r\n",
      "License\r\n",
      "included\r\n"
     ]
    }
   ],
   "source": [
    "!head -5 romeo.txt | /home/ubuntu/Project1/split.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* They worked! The .py files are attached in the .zip file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3 Part B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Edit stopwords.py with nano\n",
    "* We will use replace() to eliminate the stopwords.\n",
    "* Here are the stopwords:a, able, about, across, after, all, almost, also, am, among, an, and, any, are, as, at, be, because, been, but, by, can, cannot, could, dear, did, do, does, either, else, ever, every, for, from, get, got, had, has, have, he, her, hers, him, his, how, however, i, if, in, into, is, it, its, just, least, let, like, likely, may, me, might, most, must, my, neither, no, nor, not, of, off, often, on, only, or, other, our, own, rather, said, say, says, she, should, since, so, some, than, that, the, their, them, then, there, these, they, this, tis, to, too, twas, us, wants, was, we, were, what, when, where, which, while, who, whom, why, will, with, would, yet, you, your"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/usr/bin/env python\r\n",
      "\r\n",
      "\"\"\"\r\n",
      "A filter that remove stopwords.\r\n",
      "\"\"\"\r\n",
      "\r\n",
      "import fileinput\r\n",
      "\r\n",
      "\r\n",
      "def process(line):\r\n",
      "    \"\"\"For each line of input, replace stopwords with \" \".\"\"\"\r\n",
      "    stopwords=[\" a \",\" able \", \" about \", \" across \", \" after \", \" all \", \" almost \", \" also \", \" am \", \" among \", \" an \", \" and \", \" any \", \" are \", \" as \", \" at \", \" be \", \" because \", \" been \", \" but \", \" by \", \" can \", \" cannot \", \" could \", \" dear \", \" did \", \" do \", \" does \", \" either \", \" else \", \" ever \", \" every \", \" for \", \" from \", \" get \", \" got \", \" had \", \" has \", \" have \", \" he \", \" her \", \" hers \", \" him \", \" his \", \" how \", \" however \", \" i \", \" if \", \" in \", \" into \", \" is \", \" it \", \" its \", \" just \", \" least \", \" let \", \" like \", \" likely \", \" may \", \" me \", \" might \", \" most \", \" must \", \" my \", \" neither \", \" no \", \" nor \", \" not \", \" of \", \" off \", \" often \", \" on \", \" only \", \" or \", \" other \", \" our \", \" own \", \" rather \", \" said \", \" say \", \" says \", \" she \", \" should \", \" since \", \" so \", \" some \", \" than \", \" that \", \" the \", \" their \", \" them \", \" then \", \" there \", \" these \", \" they \", \" this \", \" tis \", \" to \", \" too \", \" twas \", \" us \", \" wants \", \" was \", \" we \", \" were \", \" what \", \" when \", \" where \", \" which \", \" while \", \" who \", \" whom \", \" why \", \" will \", \" with \", \" would \", \" yet \", \" you \", \" your \"]\r\n",
      "    line=line.strip()\r\n",
      "    line=\" \"+line+\" \"\r\n",
      "    for i in stopwords:\r\n",
      "\tline = line.replace(i,\" \")\r\n",
      "    print(line)\r\n",
      "\r\n",
      "\r\n",
      "for line in fileinput.input():\r\n",
      "    process(line)\r\n"
     ]
    }
   ],
   "source": [
    "!cat stopwords.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The Project Gutenberg EBook Romeo Juliet, William Shakespeare \r\n",
      "  \r\n",
      " This eBook use anyone anywhere cost \r\n",
      " restrictions whatsoever.  You copy it, give away \r\n",
      " re-use under terms Project Gutenberg License included \r\n",
      " eBook online www.gutenberg.org/license \r\n",
      "  \r\n",
      "  \r\n",
      " Title: Romeo Juliet \r\n",
      "  \r\n"
     ]
    }
   ],
   "source": [
    "!head -10 romeo.txt | /home/ubuntu/Project1/stopwords.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* But the elimination of these words should be case insensitive.\n",
    "* And we need to take care of the words with punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/usr/bin/env python\r\n",
      "\r\n",
      "\"\"\"\r\n",
      "A filter that remove stopwords.\r\n",
      "\"\"\"\r\n",
      "\r\n",
      "import fileinput\r\n",
      "\r\n",
      "\r\n",
      "def process(line):\r\n",
      "    \"\"\"For each line of input, replace stopwords with \" \".\"\"\"\r\n",
      "    stopwords=[\" a \",\" able \", \" about \", \" across \", \" after \", \" all \", \" almost \", \" also \", \" am \", \" among \", \" an \", \" and \", \" any \", \" are \", \" as \", \" at \", \" be \", \" because \", \" been \", \" but \", \" by \", \" can \", \" cannot \", \" could \", \" dear \", \" did \", \" do \", \" does \", \" either \", \" else \", \" ever \", \" every \", \" for \", \" from \", \" get \", \" got \", \" had \", \" has \", \" have \", \" he \", \" her \", \" hers \", \" him \", \" his \", \" how \", \" however \", \" i \", \" if \", \" in \", \" into \", \" is \", \" it \", \" its \", \" just \", \" least \", \" let \", \" like \", \" likely \", \" may \", \" me \", \" might \", \" most \", \" must \", \" my \", \" neither \", \" no \", \" nor \", \" not \", \" of \", \" off \", \" often \", \" on \", \" only \", \" or \", \" other \", \" our \", \" own \", \" rather \", \" said \", \" say \", \" says \", \" she \", \" should \", \" since \", \" so \", \" some \", \" than \", \" that \", \" the \", \" their \", \" them \", \" then \", \" there \", \" these \", \" they \", \" this \", \" tis \", \" to \", \" too \", \" twas \", \" us \", \" wants \", \" was \", \" we \", \" were \", \" what \", \" when \", \" where \", \" which \", \" while \", \" who \", \" whom \", \" why \", \" will \", \" with \", \" would \", \" yet \", \" you \", \" your \"]\r\n",
      "    line=line.lower()\r\n",
      "    line=line.strip()\r\n",
      "    line=\" \"+line+\" \"\r\n",
      "    for i in stopwords:\r\n",
      "\tline = line.replace(i,\" \")\r\n",
      "\ti = i.strip()\r\n",
      "\ti1 = \" \"+i+\",\"\r\n",
      "\ti2 = \" \"+i+\".\"\r\n",
      "\ti3 = \" \"+i+\":\"\r\n",
      "\ti4 = \" \"+i+\";\"\r\n",
      "\tline = line.replace(i1,\" \")\r\n",
      "\tline = line.replace(i2,\" \")\r\n",
      "\tline = line.replace(i3,\" \")\r\n",
      "\tline = line.replace(i4,\" \")\r\n",
      "    print(line)\r\n",
      "\r\n",
      "\r\n",
      "for line in fileinput.input():\r\n",
      "    process(line)\r\n"
     ]
    }
   ],
   "source": [
    "!cat stopwordsnew.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " project gutenberg ebook romeo juliet, william shakespeare \r\n",
      "  \r\n",
      " ebook use anyone anywhere cost \r\n",
      " restrictions whatsoever.  copy  give away \r\n",
      " re-use under terms project gutenberg license included \r\n",
      " ebook online www.gutenberg.org/license \r\n",
      "  \r\n",
      "  \r\n",
      " title: romeo juliet \r\n",
      "  \r\n"
     ]
    }
   ],
   "source": [
    "!head -10 romeo.txt | /home/ubuntu/Project1/stopwordsnew.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Till now, we get 2 filters to get rid of stopwords.\n",
    "* Use them to show the most common 25 words in Sherlock Holmes with stop words removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    467 upon\r\n",
      "    467 holmes\r\n",
      "    401 very\r\n",
      "    391 s\r\n",
      "    378 one\r\n",
      "    323 out\r\n",
      "    305 up\r\n",
      "    305 man\r\n",
      "    275 mr\r\n",
      "    269 little\r\n",
      "    234 now\r\n",
      "    232 see\r\n",
      "    230 down\r\n",
      "    201 well\r\n",
      "    183 over\r\n",
      "    175 more\r\n",
      "    174 think\r\n",
      "    171 shall\r\n",
      "    171 room\r\n",
      "    171 know\r\n",
      "    166 before\r\n",
      "    162 come\r\n",
      "    151 time\r\n",
      "    148 two\r\n",
      "    146 came\r\n",
      "sort: write failed: 'standard output': Broken pipe\r\n",
      "sort: write error\r\n"
     ]
    }
   ],
   "source": [
    "!cat holmes.txt | /home/ubuntu/Project1/stopwordsnew.py | tr -sc '[:alpha:]' '[\\n*]'| sort |  uniq -c | sort -nr |head -25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* An interesting thing is that \"s\" is in top 25 used words. Actually \"s\" comes from name's, for example Holmes's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
